{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Assignment 1 Review Report on \"Generative Adversarial Nets\".ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "TAN325r7Hz0p",
        "jkHpJTeQH3pS",
        "zfMfj7AlH8d6",
        "_LcjApdNH_qx",
        "JRdZICcQIDfX",
        "x3Hn_IrUIGYF"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS_THU3zoQOs",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on \"Generative Adversarial Nets\".ipynb\n",
        "\n",
        "Link this report in git to [Assignment 1 Review Report on \"Generative Adversarial Nets\".ipynb]()\n",
        "\n",
        "For PDF review, please go: \n",
        "\n",
        "For more associated data within assignemnt1, please go [MengXinAlex/31005]() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTg0sy35pcF8",
        "colab_type": "text"
      },
      "source": [
        "## Abstract\n",
        "\n",
        "  This report summarized a brief understanding for Generative Adversarial Nets(GAN) framework based on the paper “Generative Adversarial Nets”[3], Published by Ian J. Goodfellow, analysed its innovation within its background, criticized the advantages and disadvantages of GAN and its improvement by the following research as Deep Convolutional Generative Adversarial Network(DCGAN) and Wasserstein Generative Adversarial Network(WGAN). This report also records the explement that using the combination of DCGAN and WGAN(WDCGAN) to build a new Pokémon image generation framework."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWTM4z6-oQOt",
        "colab_type": "text"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "  On 10 Jun 2014, Ian J. Goodfellow announced and published paper “Generative Adversarial Nets”, and exerted a great impact in Computer science circle, especially for Machine learning area. A new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution and a discriminative model D that estimates the procedure for G is to maximize the probability of D making a mistake. This the review report, it explained the content of GAN framework, related work and theoretical prove, stated the innovation of GAN, the problem solving and its functionality. It also analysed the technology that been used, the extended application and later research achievement based on GAN framework within this paper. Furthermore, the quality of the presentation of paper has been mentioned as well.\n",
        " \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwEV5eoeoQOu",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1azrN3hgoQOv",
        "colab_type": "text"
      },
      "source": [
        "In Ian J. Goodfellow’s paper “Generative Adversarial Nets”, It proposed a new framework for estimating generative models using an adversarial process. The main idea of GAN is essentially two networks that are trying to win a game. The generator network tries to generate samples that come from the same distribution of the training data while the discriminator network tries to determine whether the samples are real or fake. The generator network is trying to win this game by tricking the discriminator into thinking samples are real when they are in fact fake. The discriminator network is trying to win the game by correctly telling the real samples from the fake samples. Iteratively training these networks will eventually result in a generator network capable of creating samples that cannot be discerned from real samples.[1] \n",
        " \n",
        " \n",
        "In Goodfellow’s paper, the discriminator D and generator G can be presented in this way: \n",
        "\n",
        "![GAN formula](https://raw.githubusercontent.com/MengXinAlex/31005/master/asm1%20static/2.png)\n",
        "\n",
        "Where V (D, G) represents for the WGAN framework, D represents for the discriminator model and G represents for the generator model. First, G would start creating data from  a prior on input noise variables $p_g(x)$, then D and G would take round to be trained to gain a better performance. This formula shown that G tries to minimize V in order to cheat G and generate the data that close to the real one, while D aims to maximize V to achieve a better performance on distinguishing the fake data. After the formula has been proved theoretically, it’s shown that when G. reconverting the training data distribution and D equal to 1/2 everywhere, the result would be assumed as optimal solution.\n",
        "\n",
        "This image have a well of explained the process of GAN. [4]\n",
        "\n",
        "\n",
        "\n",
        "![explain GAN](https://raw.githubusercontent.com/MengXinAlex/31005/master/asm1%20static/1.png)\n",
        "\n",
        "\n",
        "As this paper mentioned in Advantages and disadvantages, there is no explicit representation of $p_g(x)$, this would cause the parameters of the model to oscillate, destabilize and never converge. What’s more, a hard designed generator that using GAN may collapse due to its complexity, which lead limited varieties of samples. It the discriminator gets too successful, the generator gradient will vanish and learning, on another hand, the gradient of the model is not easy to control within the GAN framework that designed in this paper[5]. Recent years, more and more GAN based framework are figured out, and it would be shown in the next section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElN7zRpYoQOw",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp-_sj3VoQOx",
        "colab_type": "text"
      },
      "source": [
        "In the paper, two things attracted me the most: One is the impact of GAN framework, such as application, another is the algorithm upgrading and innovated framework. \n",
        " \n",
        "After the GAN framework has been created, it causes a great flutter in the deep learning area. This paper contributed a new methodology for deep learning that effective representation method of data distribution, it produces samples from the estimated data distribution from a period of random noises. The distinctive way of deep learning framework inspired many computer scientists from 2014 until now. Compare with the traditional deep learning model, It has many advantages such as no Markov chains needed, only backdrop is used to obtain gradients, no inference needed during learning, and a wide variety of functions within the model. In another hand, many problems occurred in GAN framework that needs to be solved. \n",
        "For recent years, several new algorithms were created based on GAN such as DCGAN and WGAN[9], which successfully fixed the existing problem. As we mentioned the problem within the GAN framework, DCGAN is definitely improved the performance of GAN that we can improve our model by decrease the loss from the generator and discriminator[1], but another problem is that the generated images are very similar to each other. To solve this problem. Finally, WGAN has created by Arjovsky[2]. Compare with GAN, WGAN does not use the sigmoid, does not do the log calculation on the loss of generator and discriminator, and the most important is, WGAN is using Wasserstein distance to decrease the loss so that we can use it to measure and control the gradient of the model.\n",
        " \n",
        "In the experiment section of this report, I used A Wasserstein Deep Convolutional Generative Adversarial Network (WDCGAN) that created a new Pokemon image generation framework, captured a better understanding for this area of study."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BIPe-FVoQOy",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RztPJIh5oQOz",
        "colab_type": "text"
      },
      "source": [
        "Following the introduction of the paper, the author introduced the traditional deep learning model and its related training method, training process and relevant terminology to build a brief understanding of the background. Then, he introduced the concept idea of the adversarial net, explained the proposed idea and theorised the GAN framework to theorem formula, described the process in detail by using 4 different situations of the framework to tell the relationship between data, discriminative model, generative model and model gradient, pointing the stated the unique solution of the framework is when $ D(x)  = 1/2 $ Then, he proved the efficacy and feasibility of the framework in methodology by proof of work, after the formal prove, he conferred that the frame still has a lack in theoretically, but could still build up a reasonable model to support his thesis. After that, the paper demonstrated the experiments using GAN framework, including MNIST,  Toronto Face Database(TFD) and CIFAR-10. The results of experiments proved the propose of GAN framework, signed the success of his research. In the end, the author summarized the advantages of GAN framework, he also analysed the limitations within his work, encourages more research to be done to improve the framework. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADNu7FcDoQO0",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-jiouQ2oQO0",
        "colab_type": "text"
      },
      "source": [
        "Until today, The paper \"Generative Adversarial Nets\" has been published 5 years, except for the improvement of the framework, the methodology of the adversarial net itself has been applied in a broader domain. In 2018, NVIDIA proposed an alternative generator architecture for GAN that draws insights from style transfer techniques[7]. As for myself, I’m more interest in exploring the improvement of the framework itself. To test for the performance of the improved GAN framework, I have designed a new Pokemon image generation framework based on Wasserstein Deep Convolutional Generative Adversarial Network (WDCGAN) that including data augmentation, a generative model G that captures the data distribution, a discriminative model D that estimates the probability that a sample came from the training data rather than G and simultaneously training these two models together. In this experiemnt, it is going to show the advantages of Wasserstein Generative Adversarial Network (WGAN) and how it combines with CNN to become WDCGAN. The method to achieve the implantation would be shown and I would discuss the result in the experiment and the conclusion of the total framework.\n",
        "\n",
        "For more under standing, Please please please chack the experiment section on github: *MengXinAlex/31005*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDA7OzH6oQO1",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0abHlqdYoQO2",
        "colab_type": "text"
      },
      "source": [
        "In this paper, It is easy for an IT student even without a ML background to understand the concept of the framework, Goodfellow used many analogising  in order to explain the general idea of his work, the author explained the models within his work by analogising the generative model as a team of counterfeiter and analogising the discriminative model as the police. As for academically reading, he also provided a methodology formula to explain GAN framework and theoretical proves for its feasibility. In section2, Related work, it’s pretty hard for people without expert machine learning reader to understand. It would be much helpful if the paper could explain more specific on the relationship between GAN framework and the models/methods that being mentioned. As for the reader who had a good understanding with relevant knowledge, the Related Work is exactly allocated on the right position and correctly mentioned all the relevant methods that have been used or related to GAN framework. Other than the technical problem that accrued within the algorithm design and been solved in the 2ed edition of this paper, its structure is well designed with a logically related order. In this way, the reader could follow the concept from general ideas to an academic understanding. To be honest, The overall structure of this paper is clear and ordered well. This paper is a really good example with excellent content for learning how to write a paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wimBrQvN7xE9",
        "colab_type": "text"
      },
      "source": [
        "## Experiment\n",
        "\n",
        "\n",
        "After the study of GAN framework, I have designed a new Pokemon image generation framework based on Wasserstein Deep Convolutional Generative Adversarial Network (WDCGAN) that including data augmentation, a generative model G that captures the data distribution, a discriminative model D that estimates the probability that a sample came from the training data rather than G and simultaneously training these two models together. In this experiemnt, it is going to show the advantages of Wasserstein Generative Adversarial Network (WGAN) and how it combines with CNN to become WDCGAN. The method to achieve the implantation would be shown and I would discuss the result in the experiment and the conclusion of the total framework.\n",
        "\n",
        "Please please please chack this section on github: *MengXinAlex/31005*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkC5gr4NDho2",
        "colab_type": "text"
      },
      "source": [
        "### All related code work are functional and have ability to test for.\n",
        "\n",
        "The original data processed on Xianfeng Zhuge's Google Drive transferred to MengXinAlex/31005. To run this code, you have to connect the Colab to your google drive and copy the data into your google drive as well.\n",
        "\n",
        "Training on Colab will not be suggested, as for my works spend for weeks.\n",
        "\n",
        "First, the prepare data function are created to resize and unify the type and scale of data and copy it for 5 times, then, the process data function would change the feature of the data to achieve augmentation the data, convert them into TFRecord trough the pipeline and prepare for training. The generator function was created with deconvolution. The feature map dimensions after going through deconvolution layersis4 4 512,8 8 256,16 16 128,32 32 64,64 64 32and128 128 3. There are 6 deconvolution layers designed in the generator. On the contrary, the discriminator function is using convolution neural network to train the discriminator against the generator. Finally, in training function, the calculation for Wasserstein distance and find the reduce the loss by calling generator and discriminator in the number of epoch that have been settled, saved the model for every 500 epoch and save a picture for every 50 epoch.\n",
        "\n",
        "Please be respect to my work :)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jZNj5pVDyCE",
        "colab_type": "text"
      },
      "source": [
        "### Data preparation\n",
        "\n",
        "\n",
        "The Neural networks need to have enough training samples to get a well perfor- mance. As for the data that trained in this framework, Pokemon picture datasets have been found in Kaggle and other website[6]. The biggest number of data in the data set are only 920 pictures, in order to have a better result, data augmenta- tions are used to improve the quality of result. The tool that have used to achieve this task are Tensorflow and openCV. prepare data and process data functions are designed for this part.\n",
        "At first, we need to resize our data and unify the format of the data. The resize function in CV2 has been used in this step. After several test for the frame work, the final size of the picture are 128*128 and the resized dataset is stored in resized data file. In prepare data function, the data has been copied for 5 times during the resize step, prepare for the data augmentation in the next step.\n",
        "\n",
        "Some of pictures in the dataset are in form of RGBA, some errors are occurred with the datasets, so the dataset are checked to change to RGB and saved in jpg format. In the process data function, the dataset was decoded and convert to TFRecord by building up the pipeline. Then, functions random flip left right, random flip up down, random brightness and random contrast in tf.image pack- age are used as data augmentation techniques to change the picture with ran- dom feature. random flip left right is flip the picture left and right randomly, ran- dom flip up down is to flip the picture up and down randomly, random brightness is change the brightness of the picture randomly, and random contrast is adjust the contrast of an image by a random factor. after these function, the data are resized and changed back to float type. Then, the data were separated randomly batch by batch with the size of 64 by 4 thread, it was import for uniform distribution and it is going to be discussed later. The images batch and the number of images were returned in the process data function. Finally, all the data preparation were done."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAN325r7Hz0p",
        "colab_type": "text"
      },
      "source": [
        "#### Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB5nuBCF6nw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXewV9jb6p_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls \"/content/drive/My Drive/pokemn/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJZaH0RK6qbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare data\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import scipy.misc\n",
        "from PIL import Image\n",
        "\n",
        "HEIGHT, WIDTH, CHANNEL = 128, 128, 3\n",
        "BATCH_SIZE = 64\n",
        "EPOCH = 50\n",
        "\n",
        "def prepare_data():\n",
        "\n",
        "#     #resize the data\n",
        "#     src = \"/content/drive/My Drive/pokemn/data\"\n",
        "#     dst = \"/content/drive/My Drive/pokemn/resizedData\"\n",
        "#     for i in range(5):\n",
        "#         for each in os.listdir(src):\n",
        "#             if \"DS_Store\" in each:\n",
        "#                 continue\n",
        "#             img = cv2.imread(os.path.join(src,each))\n",
        "#             img = cv2.resize(img,(128,128))\n",
        "#             cv2.imwrite(os.path.join(dst,str(i)+each), img)\n",
        "\n",
        "    #convert RGBA to RGB\n",
        "    src = \"/content/drive/My Drive/pokemn/resizedData\"\n",
        "    dst = \"/content/drive/My Drive/pokemn/resized_black/\"\n",
        "    for each in os.listdir(src):\n",
        "        if \"DS_Store\" in each:\n",
        "            continue\n",
        "        png = Image.open(os.path.join(src,each))\n",
        "        if png.mode == 'RGBA':\n",
        "            png.load()\n",
        "            background = Image.new(\"RGB\", png.size, (0,0,0))\n",
        "            background.paste(png, mask=png.split()[3]) # 3 is the alpha channel\n",
        "            background.save(os.path.join(dst,each.split('.')[0] + '.jpg'), 'JPEG')\n",
        "        else:\n",
        "            png.convert('RGB')\n",
        "            png.save(os.path.join(dst,each.split('.')[0] + '.jpg'), 'JPEG')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6NzJuHv7T0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prepare_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEAvJVDbEDcl",
        "colab_type": "text"
      },
      "source": [
        "### Useful Method\n",
        "\n",
        "Some useful method to store and visualize the training result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkHpJTeQH3pS",
        "colab_type": "text"
      },
      "source": [
        "#### Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5shSZbt-EpOp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "# generate new kinds of pokemons\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import random\n",
        "import scipy.misc\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "\n",
        "HEIGHT, WIDTH, CHANNEL = 128, 128, 3\n",
        "BATCH_SIZE = 64\n",
        "EPOCH = 300\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '15'\n",
        "version = 'newPokemon'\n",
        "newPoke_path = './' + version\n",
        "\n",
        "def lrelu(x, n, leak=0.2):\n",
        "    return tf.maximum(x, leak * x, name=n)\n",
        "\n",
        "def process_data():\n",
        "    current_dir =  \"/content/drive/My Drive/pokemn/\"#os.getcwd()\n",
        "    # parent = os.path.dirname(current_dir)\n",
        "    pokemon_dir = os.path.join(current_dir, 'resizedData')\n",
        "    images = []\n",
        "    for each in os.listdir(pokemon_dir):\n",
        "        images.append(os.path.join(pokemon_dir,each))\n",
        "    # print images\n",
        "    all_images = tf.convert_to_tensor(images, dtype = tf.string)\n",
        "\n",
        "    images_queue = tf.train.slice_input_producer(\n",
        "                                        [all_images])\n",
        "\n",
        "    content = tf.read_file(images_queue[0])\n",
        "    image = tf.image.decode_jpeg(content, channels = CHANNEL)\n",
        "    # sess1 = tf.Session()\n",
        "    # print sess1.run(image)\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_brightness(image, max_delta = 0.1)\n",
        "    image = tf.image.random_contrast(image, lower = 0.9, upper = 1.1)\n",
        "    # noise = tf.Variable(tf.truncated_normal(shape = [HEIGHT,WIDTH,CHANNEL], dtype = tf.float32, stddev = 1e-3, name = 'noise'))\n",
        "    # print image.get_shape()\n",
        "    size = [HEIGHT, WIDTH]\n",
        "    image = tf.image.resize_images(image, size)\n",
        "    image.set_shape([HEIGHT,WIDTH,CHANNEL])\n",
        "    # image = image + noise\n",
        "    # image = tf.transpose(image, perm=[2, 0, 1])\n",
        "    # print image.get_shape()\n",
        "\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = image / 255.0\n",
        "\n",
        "    iamges_batch = tf.train.shuffle_batch(\n",
        "                                    [image], batch_size = BATCH_SIZE,\n",
        "                                    num_threads = 4, capacity = 200 + 3* BATCH_SIZE,\n",
        "                                    min_after_dequeue = 200)\n",
        "    num_images = len(images)\n",
        "\n",
        "    return iamges_batch, num_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_26vdHZFgdf",
        "colab_type": "text"
      },
      "source": [
        "### Define Generator Model\n",
        "\n",
        "\n",
        "In the generator function, the network has 6 convolutional layers using tf.layers.conv2d transpose, all followed by batch normalization, tf.contrib.layers.batch norm and Rectified Lin-\n",
        "ear unit (ReLU) tf.nn.relut except the last out put layer. Each upsampling layer\n",
        "represents a transpose convolution operation with strides 2. Transpose convolutions\n",
        "are similar to the regular convolutions. The generator begins with deep and narrow vectors, transpose convolution use 5x5 kernel’s size depths reducing from 512 to 3, using a deconvolution network to train the generator. all the parameters have been settled in tf.layers.conv2d transpose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfMfj7AlH8d6",
        "colab_type": "text"
      },
      "source": [
        "#### Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ_nxVwA7UeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Generator Model\n",
        "\n",
        "\n",
        "def generator(input, random_dim, is_train, reuse=False):\n",
        "    c4, c8, c16, c32, c64 = 512, 256, 128, 64, 32 # channel num\n",
        "    s4 = 4\n",
        "    output_dim = CHANNEL  # RGB image\n",
        "    with tf.variable_scope('gen', reuse = tf.AUTO_REUSE) as scope:\n",
        "        if reuse:\n",
        "            scope.reuse_variables()\n",
        "        w1 = tf.get_variable('w1', shape=[random_dim, s4 * s4 * c4], dtype=tf.float32, initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "        b1 = tf.get_variable('b1', shape=[c4 * s4 * s4], dtype=tf.float32,\n",
        "                             initializer=tf.constant_initializer(0.0))\n",
        "        flat_conv1 = tf.add(tf.matmul(input, w1), b1, name='flat_conv1')\n",
        "        # 4*4*512\n",
        "        conv1 = tf.reshape(flat_conv1, shape=[-1, s4, s4, c4], name='conv1')\n",
        "        bn1 = tf.contrib.layers.batch_norm(conv1, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn1')\n",
        "        act1 = tf.nn.relu(bn1, name='act1')\n",
        "        # 8*8*256\n",
        "        conv2 = tf.layers.conv2d_transpose(act1, c8, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
        "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                                           name='conv2')\n",
        "        bn2 = tf.contrib.layers.batch_norm(conv2, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn2')\n",
        "        act2 = tf.nn.relu(bn2, name='act2')\n",
        "        # 16*16*128\n",
        "        conv3 = tf.layers.conv2d_transpose(act2, c16, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
        "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                                           name='conv3')\n",
        "        bn3 = tf.contrib.layers.batch_norm(conv3, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn3')\n",
        "        act3 = tf.nn.relu(bn3, name='act3')\n",
        "        # 32*32*64\n",
        "        conv4 = tf.layers.conv2d_transpose(act3, c32, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
        "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                                           name='conv4')\n",
        "        bn4 = tf.contrib.layers.batch_norm(conv4, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn4')\n",
        "        act4 = tf.nn.relu(bn4, name='act4')\n",
        "        # 64*64*32\n",
        "        conv5 = tf.layers.conv2d_transpose(act4, c64, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
        "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                                           name='conv5')\n",
        "        bn5 = tf.contrib.layers.batch_norm(conv5, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn5')\n",
        "        act5 = tf.nn.relu(bn5, name='act5')\n",
        "\n",
        "        #128*128*3\n",
        "        conv6 = tf.layers.conv2d_transpose(act5, output_dim, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
        "                                           kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                                           name='conv6')\n",
        "        # bn6 = tf.contrib.layers.batch_norm(conv6, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn6')\n",
        "        act6 = tf.nn.tanh(conv6, name='act6')\n",
        "        return act6\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyCG6qtuFpat",
        "colab_type": "text"
      },
      "source": [
        "### Define Discriminator Model\n",
        "\n",
        "The discriminator is also a 6 layer CNN, tf.layers.conv2d with BN (except its input layer) and leaky ReLU activations. Many activation functions will work fine with this basic GAN architecture. However, leaky ReLUs are very popular because they help the gradients flow easier through the architecture. This is much more like a CNN, The discriminator shall be needed to output probabilities, but as for WGAN, we don’t need that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LcjApdNH_qx",
        "colab_type": "text"
      },
      "source": [
        "#### Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHMJPlUVEi-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define Discriminator Model\n",
        "\n",
        "def discriminator(input, is_train, reuse=False):\n",
        "    c2, c4, c8, c16 = 64, 128, 256, 512  # channel num: 64, 128, 256, 512\n",
        "    with tf.variable_scope('dis', reuse = tf.AUTO_REUSE) as scope:\n",
        "        if reuse:\n",
        "            scope.reuse_variables()\n",
        "        # 64*64*64\n",
        "        conv1 = tf.layers.conv2d(input, c2, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                                 name='conv1')\n",
        "        # bn1 = tf.contrib.layers.batch_norm(conv1, is_training = is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope = 'bn1')\n",
        "        act1 = lrelu(conv1, n='act1')\n",
        "        # 32*32*128\n",
        "        conv2 = tf.layers.conv2d(act1, c4, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                                 name='conv2')\n",
        "        bn2 = tf.contrib.layers.batch_norm(conv2, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn2')\n",
        "        act2 = lrelu(bn2, n='act2')\n",
        "        # 16*16*256\n",
        "        conv3 = tf.layers.conv2d(act2, c8, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                                 name='conv3')\n",
        "        bn3 = tf.contrib.layers.batch_norm(conv3, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn3')\n",
        "        act3 = lrelu(bn3, n='act3')\n",
        "        # 8*8*512\n",
        "        conv4 = tf.layers.conv2d(act3, c16, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                                 name='conv4')\n",
        "        bn4 = tf.contrib.layers.batch_norm(conv4, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn4')\n",
        "        act4 = lrelu(bn4, n='act4')\n",
        "        # # 8*8*256\n",
        "        # conv5 = tf.layers.conv2d(act4, c32, kernel_size=[5, 5], strides=[2, 2], padding=\"SAME\",\n",
        "                                 # kernel_initializer=tf.truncated_normal_initializer(stddev=0.02),\n",
        "                                 # name='conv5')\n",
        "        # bn5 = tf.contrib.layers.batch_norm(conv5, is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bn5')\n",
        "        # act5 = lrelu(bn5, n='act5')\n",
        "\n",
        "        # start from act4\n",
        "        dim = int(np.prod(act4.get_shape()[1:]))\n",
        "        fc1 = tf.reshape(act4, shape=[-1, dim], name='fc1')\n",
        "        # w1 = tf.get_variable('w1', shape=[fc1.shape[-1], 512], dtype=tf.float32,\n",
        "                             # initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "        # b1 = tf.get_variable('b1', shape=[512], dtype=tf.float32,\n",
        "                             # initializer=tf.constant_initializer(0.0))\n",
        "        # bnf = tf.contrib.layers.batch_norm(tf.matmul(fc1,w1), is_training=is_train, epsilon=1e-5, decay = 0.9,  updates_collections=None, scope='bnf')\n",
        "        # act_fc1 = lrelu(tf.nn.bias_add(bnf, b1),n = 'actf')\n",
        "\n",
        "        w2 = tf.get_variable('w2', shape=[fc1.shape[-1], 1], dtype=tf.float32,\n",
        "                             initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
        "        b2 = tf.get_variable('b2', shape=[1], dtype=tf.float32,\n",
        "                             initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "        # wgan just get rid of the sigmoid\n",
        "        logits = tf.add(tf.matmul(fc1, w2), b2, name='logits')\n",
        "        # dcgan\n",
        "        acted_out = tf.nn.sigmoid(logits)\n",
        "        return logits #, acted_out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3umkWLupF51D",
        "colab_type": "text"
      },
      "source": [
        "### Design the training process\n",
        "\n",
        "As we mentioned before, there are two main problems in GAN: the collapse mode problem and the losses are hard to decrease. As for WGAN, it solved these problem by adding noise in both generator and discriminator, in this way, the main feature from the real image would have a high probability to be learned. the clip weight would be added to decrease the noise to strength for each time discriminator has been trained, and we don’t need to worry about the discriminator would be to good at the beginning of the training[8]. The WGAN algorithm is followed:\n",
        "In the train function, the placeholders have been created first to store the real image, random input. Then, we created a fake image by the generator, test the fake image and real image separately with discriminator. then, we calculate the first D loss using the mean of fake result - mean of real result, and the G loss is negative mean fake results. Then, the noise of D and G are created, the D clip weight are created as well to decrease the noise. in the epoch loop, for each batches, it is going to train D for 5 times and G for 1 times. for a settled number of epoch, the model and the data would be stored once.\n",
        "Compare with GAN, WGAN not need to worry that D is to good for G to train and it have a better features in a batch of generated images. In GAN, the loss is hardly been decreased in gradient, and the KL and JS in the origin GAN have contradiction during the training, there is no doubt that WGAN have a better cost function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRdZICcQIDfX",
        "colab_type": "text"
      },
      "source": [
        "#### Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfRPhAN-EhJK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Design the training process\n",
        "\n",
        "def train():\n",
        "    random_dim = 100\n",
        "    tf.reset_default_graph()\n",
        "    real_image = tf.placeholder(tf.float32, shape = [None, HEIGHT, WIDTH, CHANNEL], name='real_image')\n",
        "    random_input = tf.placeholder(tf.float32, shape=[None, random_dim], name='rand_input')\n",
        "    is_train = tf.placeholder(tf.bool, name='is_train')\n",
        "\n",
        "    # wgan\n",
        "   \n",
        "    fake_image = generator(random_input, random_dim, is_train)\n",
        "    real_result = discriminator(real_image, is_train)\n",
        "    fake_result = discriminator(fake_image, is_train, reuse=True)\n",
        "\n",
        "    d_loss = tf.reduce_mean(fake_result) - tf.reduce_mean(real_result)  # This optimizes the discriminator.\n",
        "    g_loss = -tf.reduce_mean(fake_result)  # This optimizes the generator.\n",
        "\n",
        "    \n",
        "    t_vars = tf.trainable_variables()\n",
        "    d_vars = [var for var in t_vars if 'dis' in var.name]\n",
        "    g_vars = [var for var in t_vars if 'gen' in var.name]\n",
        "    # test\n",
        "    # print(d_vars)\n",
        "    trainer_d = tf.train.RMSPropOptimizer(learning_rate=2e-4).minimize(d_loss, var_list=d_vars)\n",
        "    trainer_g = tf.train.RMSPropOptimizer(learning_rate=2e-4).minimize(g_loss, var_list=g_vars)\n",
        "    # clip discriminator weights\n",
        "    d_clip = [v.assign(tf.clip_by_value(v, -0.01, 0.01)) for v in d_vars]\n",
        "\n",
        "\n",
        "    batch_size = BATCH_SIZE\n",
        "    image_batch, samples_num = process_data()\n",
        "\n",
        "    batch_num = int(samples_num / batch_size)\n",
        "    total_batch = 0\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver()\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    sess.run(tf.local_variables_initializer())\n",
        "    # continue training\n",
        "#     saver.save(sess, '/content/drive/My Drive/pokemn/model/')\n",
        "    ckpt = tf.train.latest_checkpoint('/content/drive/My Drive/pokemn/model/' + version)\n",
        "    saver.restore(sess, ckpt)\n",
        "    coord = tf.train.Coordinator()\n",
        "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
        "\n",
        "    print ('total training sample num:%d' % samples_num)\n",
        "    print ('batch size: %d, batch num per epoch: %d, epoch num: %d' % (batch_size, batch_num, EPOCH))\n",
        "    print ('start training...')\n",
        "    for i in range(EPOCH):\n",
        "        print(\"in epoch\",i)\n",
        "        for j in range(batch_num):\n",
        "            print(\"in batch\",j)\n",
        "            d_iters = 1\n",
        "            g_iters = 5\n",
        "\n",
        "            train_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n",
        "            for k in range(d_iters):\n",
        "                train_image = sess.run(image_batch)\n",
        "                #wgan clip weights\n",
        "                print(\"hello?\",k)\n",
        "                sess.run(d_clip)\n",
        "                \n",
        "                # Update the discriminator\n",
        "                _, dLoss = sess.run([trainer_d, d_loss],\n",
        "                                    feed_dict={random_input: train_noise, real_image: train_image, is_train: True})\n",
        "                print(\"yes?\")\n",
        "            # Update the generator\n",
        "            for k in range(g_iters):\n",
        "                # train_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n",
        "                _, gLoss = sess.run([trainer_g, g_loss],\n",
        "                                    feed_dict={random_input: train_noise, is_train: True})\n",
        "\n",
        "            # print 'train:[%d/%d],d_loss:%f,g_loss:%f' % (i, j, dLoss, gLoss)\n",
        "\n",
        "        # save check point every 500 epoch\n",
        "        if i%1 == 0:\n",
        "            if not os.path.exists('/content/drive/My Drive/pokemn/model/' + version):\n",
        "                os.makedirs('/content/drive/My Drive/pokemn/model/' + version)\n",
        "            saver.save(sess, '/content/drive/My Drive/pokemn/model/' +version + '/' + str(i))\n",
        "        if i%1 == 0:\n",
        "            # save images\n",
        "            if not os.path.exists(\"/content/drive/My Drive/pokemn/\"+version):\n",
        "                os.makedirs(\"/content/drive/My Drive/pokemn/\"+version)\n",
        "            sample_noise = np.random.uniform(-1.0, 1.0, size=[batch_size, random_dim]).astype(np.float32)\n",
        "            imgtest = sess.run(fake_image, feed_dict={random_input: sample_noise, is_train: False})\n",
        "            # imgtest = imgtest * 255.0\n",
        "            # imgtest.astype(np.uint8)\n",
        "            save_images(imgtest, [8,8] ,\"/content/drive/My Drive/pokemn\" + '/epoch' + str(i) + '.jpg')\n",
        "            #print ('train:[%d],d_loss:%f,g_loss:%f' % (i, dLoss, gLoss))\n",
        "    coord.request_stop()\n",
        "    coord.join(threads)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FapNgqSGWoR",
        "colab_type": "text"
      },
      "source": [
        "### Run and train the data\n",
        "\n",
        "In order to run the code in a reasonable time, the code was first run on google colaboatory, but the time was still unacceptable. so the code finally finished run by 1080Ti in 5 hours for 1500 epoch. Out of the respect, the result of the framework have a good quality level. As we can see, the result have reached our prospect."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3Hn_IrUIGYF",
        "colab_type": "text"
      },
      "source": [
        "#### Coding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0SjqhjbEdoQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run and train the data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()\n",
        "    # test()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlxdBFveGgLf",
        "colab_type": "text"
      },
      "source": [
        "### Result\n",
        "\n",
        "Although the WGAN framework that I made have a bad running time issue, but the good result for the data would be a good new for that. The running time may improve after this assignment. As for the Wasserstein GAN algorithm, it has solved the collapse mode problem and loss decreasing by a little change on GAN, but improved a big step for processing of Generative Adversarial Network.\n",
        "\n",
        "Here is a result example:\n",
        "\n",
        "![alt text](https://raw.githubusercontent.com/MengXinAlex/31005/master/newPokemon/epoch020.jpg)\n",
        "\n",
        "\n",
        "Another one: \n",
        "\n",
        "\n",
        "![result1](https://raw.githubusercontent.com/MengXinAlex/31005/master/newPokemon/epoch021.jpg)\n",
        "\n",
        "* Above two images © 2018-2019 Xianfeng Zhuge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5Y44LfSIgNm",
        "colab_type": "text"
      },
      "source": [
        "For more result picture, please check : [MengXinAlex/31005/newPokemon](https://github.com/MengXinAlex/31005/tree/master/newPokemon)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNvjxuC-oQO3",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[1][Arjovsky and Bottou, 2018a][1]: Arjovsky, M. and Bottou, L. 2018a.Towards Principled Methods for Training Generative Adversarial Networks. [online] Available at: *https://arxiv.org/abs/1701.04862* [Accessed 21 Aug. 2019]\n",
        "\n",
        "[1]: https://arxiv.org/abs/1701.04862\n",
        "\n",
        "\n",
        "[2][Arjovsky and Bottou, 2018b][2]: Arjovsky, M. and Bottou, L. 2018b, Wasserstein generative adversarial networks, *International conference on machine learning*, pp. 214-223.\n",
        "\n",
        "[2]: https://arxiv.org/abs/1701.07875\n",
        "\n",
        "\n",
        "[3][GOO14][3]: Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. and Bengio, Y. 2014, Generative Adversarial Networks, *Advances in Neural Information Processing Systems 27*, Curran Associates, pp. 2672–2680.\n",
        "\n",
        "[3]: https://arxiv.org/abs/1406.2661\n",
        "\n",
        "\n",
        "[4][Goodfellow, 2016][5]: Goodfellow, I. 2016, Generative Adversarial Networks (GANs). [Presentation] Availiable at: *https://media.nips.cc/Conferences/2016/Slides/6202-Slides.pdf*\n",
        "[4]: https://media.nips.cc/Conferences/2016/Slides/6202-Slides.pdf\n",
        "\n",
        "\n",
        "[5][Hui, 2018][5]: Hui, J. (2018). GAN — Why it is so hard to train Generative Adversarial Networks!. [online] Medium. Available at: *https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b* [Accessed 29 Aug. 2019].\n",
        "\n",
        "[5]: https://medium.com/@jonathan_hui/gan-why-it-is-so-hard-to-train-generative-advisory-networks-819a86b3750b\n",
        "\n",
        "\n",
        "[6][Kaggle.com. 2019][6]: Kaggle.com. 2019, The Complete Pokemon Dataset. [online] Available at: https://www.kaggle.com/rounakbanik/pokemon [Accessed 21 Aug. 2019].\n",
        "\n",
        "[6]: https://www.kaggle.com/rounakbanik/pokemon\n",
        "\n",
        "\n",
        "[7][Medium, 2019][7]: Medium. (2019). GAN 2.0: NVIDIA’s Hyperrealistic Face Generator. [online] Available at: *https://medium.com/syncedreview/gan-2-0-nvidias-hyperrealistic-face-generator-e3439d33ebaf* [Accessed 29 Aug. 2019].\n",
        "\n",
        "[7]: https://medium.com/syncedreview/gan-2-0-nvidias-hyperrealistic-face-generator-e3439d33ebaf\n",
        "\n",
        "[8][Radford et al. 2018][8]: Radford, A., Metz, L. and Chintala, S. 2018, Unsupervised Representa- tion Learning with Deep Convolutional Generative Adversarial Networks. [online] Arxiv.org. Available at: *https://arxiv.org/abs/1511.06434* [Accessed 25 Aug. 2019].\n",
        "\n",
        "[8]: https://arxiv.org/abs/1511.06434\n",
        "\n",
        "\n",
        "\n",
        "[9][Stephenhky, 2018][9]: Stephenhky, V. (2018). Everything about Data Analytics.Available at: *https://datawarrior.wordpress.com/2017/03/30/on-wasserstein-gan* [Accessed 27 Aug. 2019].\n",
        "\n",
        "[9]: https://datawarrior.wordpress.com/2017/03/30/on-wasserstein-gan\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}